"""
Exploratory Data Analysis (EDA) for retail sales forecasting.

This script performs comprehensive EDA on the data splits and generates:
- Visualizations (plots saved to reports/figures/)
- Summary tables (CSVs saved to reports/tables/)
- Dashboard-ready aggregates

All outputs are repeatable and can be regenerated by running this script.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.config import get_splits_dir, get_reports_dir

# Set style for better-looking plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Create output directories (overridable via REPORTS_DIR on SageMaker)
REPORTS_DIR = get_reports_dir()
FIGURES_DIR = REPORTS_DIR / "eda" / "figures"
TABLES_DIR = REPORTS_DIR / "eda" / "tables"

FIGURES_DIR.mkdir(parents=True, exist_ok=True)
TABLES_DIR.mkdir(parents=True, exist_ok=True)


def load_data():
    """Load train, validation, and test splits (path overridable via SPLITS_DIR on SageMaker)."""
    print("üì• Loading data splits...")
    
    data_dir = get_splits_dir()
    
    train_df = pd.read_csv(data_dir / "train.csv")
    val_df = pd.read_csv(data_dir / "val.csv")
    test_df = pd.read_csv(data_dir / "test.csv")
    
    # Convert dates
    for df in [train_df, val_df, test_df]:
        df['week_date'] = pd.to_datetime(df['week_date'])
    
    print(f"  ‚úÖ Train: {len(train_df):,} rows")
    print(f"  ‚úÖ Validation: {len(val_df):,} rows")
    print(f"  ‚úÖ Test: {len(test_df):,} rows")
    
    # Combine for full dataset analysis
    full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)
    full_df = full_df.sort_values(['store_id', 'dept_id', 'week_date'])
    
    return train_df, val_df, test_df, full_df


def schema_sanity_checks(full_df):
    """Perform schema and data quality checks."""
    print("\n" + "=" * 60)
    print("üîç Schema Sanity Checks")
    print("=" * 60)
    
    results = {}
    
    # 1. Data types
    print("\nüìã Data Types:")
    print(full_df.dtypes)
    results['dtypes'] = full_df.dtypes.to_dict()
    
    # 2. Missing values
    print("\n‚ùì Missing Values:")
    missing = full_df.isnull().sum()
    missing_pct = (missing / len(full_df) * 100).round(2)
    missing_df = pd.DataFrame({
        'missing_count': missing,
        'missing_pct': missing_pct
    })
    missing_df = missing_df[missing_df['missing_count'] > 0]
    if len(missing_df) > 0:
        print(missing_df)
        results['missing_values'] = missing_df.to_dict()
    else:
        print("  ‚úÖ No missing values!")
        results['missing_values'] = {}
    
    # 3. Duplicates
    print("\nüîÑ Duplicate Rows:")
    dup_cols = ['store_id', 'dept_id', 'week_date']
    duplicates = full_df.duplicated(subset=dup_cols).sum()
    print(f"  Duplicate (store, dept, date) combinations: {duplicates}")
    results['duplicates'] = duplicates
    
    # 4. Negative sales
    print("\n‚ö†Ô∏è  Negative Weekly Sales:")
    negative_sales = full_df[full_df['weekly_sales'] < 0]
    print(f"  Rows with negative sales: {len(negative_sales):,} ({len(negative_sales)/len(full_df)*100:.2f}%)")
    if len(negative_sales) > 0:
        print(f"  Min sales: ${full_df['weekly_sales'].min():,.2f}")
        print(f"  Negative sales summary:")
        print(negative_sales[['store_id', 'dept_id', 'week_date', 'weekly_sales']].head(10))
        results['negative_sales_count'] = len(negative_sales)
        results['negative_sales_pct'] = len(negative_sales) / len(full_df) * 100
    else:
        print("  ‚úÖ No negative sales!")
        results['negative_sales_count'] = 0
    
    # 5. Outliers (using IQR method)
    print("\nüìä Outlier Detection (IQR method):")
    Q1 = full_df['weekly_sales'].quantile(0.25)
    Q3 = full_df['weekly_sales'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = full_df[(full_df['weekly_sales'] < lower_bound) | (full_df['weekly_sales'] > upper_bound)]
    print(f"  Outliers (outside 1.5*IQR): {len(outliers):,} ({len(outliers)/len(full_df)*100:.2f}%)")
    results['outliers_count'] = len(outliers)
    results['outliers_pct'] = len(outliers) / len(full_df) * 100
    
    # Save summary
    summary_df = pd.DataFrame([results])
    summary_df.to_csv(TABLES_DIR / "schema_sanity_summary.csv", index=False)
    print(f"\n  üíæ Saved: {TABLES_DIR / 'schema_sanity_summary.csv'}")
    
    return results


def time_series_shape_analysis(full_df):
    """Analyze time series patterns and shapes."""
    print("\n" + "=" * 60)
    print("üìà Time Series Shape Analysis")
    print("=" * 60)
    
    # 1. Total sales over time
    print("\nüìä Total Sales Over Time...")
    weekly_totals = full_df.groupby('week_date')['weekly_sales'].sum().reset_index()
    weekly_totals.columns = ['week_date', 'total_sales']
    
    plt.figure(figsize=(14, 6))
    plt.plot(weekly_totals['week_date'], weekly_totals['total_sales'], linewidth=2)
    plt.title('Total Weekly Sales Over Time', fontsize=16, fontweight='bold')
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Total Sales ($)', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "total_sales_over_time.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'total_sales_over_time.png'}")
    
    weekly_totals.to_csv(TABLES_DIR / "weekly_total_sales.csv", index=False)
    
    # 2. Sales by store_type
    print("\nüè™ Sales by Store Type...")
    store_type_sales = full_df.groupby(['week_date', 'store_type'])['weekly_sales'].sum().reset_index()
    store_type_sales = store_type_sales.pivot(index='week_date', columns='store_type', values='weekly_sales')
    
    plt.figure(figsize=(14, 6))
    for store_type in store_type_sales.columns:
        plt.plot(store_type_sales.index, store_type_sales[store_type], 
                label=f'Type {store_type}', linewidth=2)
    plt.title('Total Sales by Store Type Over Time', fontsize=16, fontweight='bold')
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Total Sales ($)', fontsize=12)
    plt.legend(title='Store Type')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "sales_by_store_type.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'sales_by_store_type.png'}")
    
    store_type_sales.to_csv(TABLES_DIR / "sales_by_store_type.csv")
    
    # 3. Sales by store_id (top 10 stores)
    print("\nüè¨ Top 10 Stores by Total Sales...")
    store_totals = full_df.groupby('store_id')['weekly_sales'].sum().sort_values(ascending=False).head(10)
    
    plt.figure(figsize=(12, 6))
    store_totals.plot(kind='bar')
    plt.title('Top 10 Stores by Total Sales', fontsize=16, fontweight='bold')
    plt.xlabel('Store ID', fontsize=12)
    plt.ylabel('Total Sales ($)', fontsize=12)
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "top_stores_by_sales.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'top_stores_by_sales.png'}")
    
    store_totals.to_csv(TABLES_DIR / "top_stores_by_sales.csv")
    
    # 4. Sales by dept_id (top 15 departments)
    print("\nüì¶ Top 15 Departments by Total Sales...")
    dept_totals = full_df.groupby('dept_id')['weekly_sales'].sum().sort_values(ascending=False).head(15)
    
    plt.figure(figsize=(14, 6))
    dept_totals.plot(kind='bar')
    plt.title('Top 15 Departments by Total Sales', fontsize=16, fontweight='bold')
    plt.xlabel('Department ID', fontsize=12)
    plt.ylabel('Total Sales ($)', fontsize=12)
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "top_depts_by_sales.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'top_depts_by_sales.png'}")
    
    dept_totals.to_csv(TABLES_DIR / "top_depts_by_sales.csv")
    
    # 5. Holiday vs Non-Holiday distributions
    print("\nüéâ Holiday vs Non-Holiday Sales...")
    holiday_sales = full_df[full_df['isholiday'] == True]['weekly_sales']
    non_holiday_sales = full_df[full_df['isholiday'] == False]['weekly_sales']
    
    plt.figure(figsize=(12, 6))
    plt.hist(non_holiday_sales, bins=50, alpha=0.7, label='Non-Holiday', density=True)
    plt.hist(holiday_sales, bins=50, alpha=0.7, label='Holiday', density=True)
    plt.xlabel('Weekly Sales ($)', fontsize=12)
    plt.ylabel('Density', fontsize=12)
    plt.title('Sales Distribution: Holiday vs Non-Holiday', fontsize=16, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "holiday_vs_nonholiday_dist.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'holiday_vs_nonholiday_dist.png'}")
    
    holiday_summary = pd.DataFrame({
        'type': ['Holiday', 'Non-Holiday'],
        'count': [len(holiday_sales), len(non_holiday_sales)],
        'mean_sales': [holiday_sales.mean(), non_holiday_sales.mean()],
        'median_sales': [holiday_sales.median(), non_holiday_sales.median()],
        'std_sales': [holiday_sales.std(), non_holiday_sales.std()]
    })
    holiday_summary.to_csv(TABLES_DIR / "holiday_summary.csv", index=False)
    
    # 6. Markdown availability (pre/post Nov 2011)
    print("\nüí∞ Markdown Availability Analysis...")
    full_df['year_month'] = full_df['week_date'].dt.to_period('M')
    markdown_cols = ['markdown1', 'markdown2', 'markdown3', 'markdown4', 'markdown5']
    
    # Check when markdowns start
    markdown_available = full_df[markdown_cols].notna().any(axis=1)
    # Calculate percentage with markdowns per month
    markdown_by_month = (
        full_df.groupby('year_month')[markdown_cols]
        .apply(lambda x: x.notna().any(axis=1).sum() / len(x) * 100)
        .reset_index(name='pct_with_markdown')
    )
    markdown_by_month['year_month'] = markdown_by_month['year_month'].astype(str)
    
    plt.figure(figsize=(14, 6))
    plt.plot(markdown_by_month['year_month'], markdown_by_month['pct_with_markdown'], 
            marker='o', linewidth=2, markersize=8)
    plt.title('Percentage of Weeks with Markdowns Over Time', fontsize=16, fontweight='bold')
    plt.xlabel('Year-Month', fontsize=12)
    plt.ylabel('Percentage (%)', fontsize=12)
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    plt.axvline(x='2011-11', color='r', linestyle='--', alpha=0.7, label='Nov 2011')
    plt.legend()
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "markdown_availability.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'markdown_availability.png'}")
    
    # Sales with vs without markdowns
    full_df['has_markdown'] = full_df[markdown_cols].notna().any(axis=1)
    markdown_sales = full_df[full_df['has_markdown'] == True]['weekly_sales']
    no_markdown_sales = full_df[full_df['has_markdown'] == False]['weekly_sales']
    
    markdown_comparison = pd.DataFrame({
        'type': ['With Markdown', 'Without Markdown'],
        'count': [len(markdown_sales), len(no_markdown_sales)],
        'mean_sales': [markdown_sales.mean(), no_markdown_sales.mean()],
        'median_sales': [markdown_sales.median(), no_markdown_sales.median()]
    })
    markdown_comparison.to_csv(TABLES_DIR / "markdown_sales_comparison.csv", index=False)
    
    markdown_by_month.to_csv(TABLES_DIR / "markdown_availability_by_month.csv", index=False)


def series_level_diagnostics(full_df):
    """Analyze series-level characteristics."""
    print("\n" + "=" * 60)
    print("üî¨ Series-Level Diagnostics")
    print("=" * 60)
    
    # 1. Number of weeks per (store, dept)
    print("\nüìÖ Weeks Available per Store-Dept Combination...")
    weeks_per_series = full_df.groupby(['store_id', 'dept_id']).size().reset_index(name='n_weeks')
    
    plt.figure(figsize=(12, 6))
    plt.hist(weeks_per_series['n_weeks'], bins=30, edgecolor='black')
    plt.xlabel('Number of Weeks', fontsize=12)
    plt.ylabel('Number of Store-Dept Combinations', fontsize=12)
    plt.title('Distribution of Weeks Available per Store-Dept Series', fontsize=16, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "weeks_per_series_dist.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'weeks_per_series_dist.png'}")
    
    weeks_per_series.to_csv(TABLES_DIR / "weeks_per_series.csv", index=False)
    
    # 2. Cold-start groups (short history)
    print("\n‚ùÑÔ∏è  Cold-Start Analysis...")
    cold_start_threshold = 26  # Less than 6 months
    cold_start = weeks_per_series[weeks_per_series['n_weeks'] < cold_start_threshold]
    print(f"  Store-Dept combinations with < {cold_start_threshold} weeks: {len(cold_start):,}")
    print(f"  Percentage: {len(cold_start)/len(weeks_per_series)*100:.2f}%")
    
    cold_start.to_csv(TABLES_DIR / "cold_start_series.csv", index=False)
    
    # 3. Volatility buckets
    print("\nüìä Volatility Analysis...")
    series_stats = full_df.groupby(['store_id', 'dept_id']).agg({
        'weekly_sales': ['mean', 'std', 'count']
    }).reset_index()
    series_stats.columns = ['store_id', 'dept_id', 'mean_sales', 'std_sales', 'n_weeks']
    series_stats['cv'] = series_stats['std_sales'] / series_stats['mean_sales']  # Coefficient of variation
    
    # Bucket by volatility
    series_stats['volatility_bucket'] = pd.cut(
        series_stats['cv'],
        bins=[0, 0.3, 0.6, 1.0, np.inf],
        labels=['Low (CV<0.3)', 'Medium (0.3‚â§CV<0.6)', 'High (0.6‚â§CV<1.0)', 'Very High (CV‚â•1.0)']
    )
    
    volatility_counts = series_stats['volatility_bucket'].value_counts().sort_index()
    
    plt.figure(figsize=(10, 6))
    volatility_counts.plot(kind='bar')
    plt.title('Series Volatility Distribution', fontsize=16, fontweight='bold')
    plt.xlabel('Volatility Bucket', fontsize=12)
    plt.ylabel('Number of Series', fontsize=12)
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "volatility_buckets.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ‚úÖ Saved: {FIGURES_DIR / 'volatility_buckets.png'}")
    
    series_stats.to_csv(TABLES_DIR / "series_statistics.csv", index=False)
    volatility_counts.to_csv(TABLES_DIR / "volatility_counts.csv")


def dashboard_ready_aggregates(full_df):
    """Create dashboard-ready aggregate tables."""
    print("\n" + "=" * 60)
    print("üìä Dashboard-Ready Aggregates")
    print("=" * 60)
    
    # 1. Weekly total sales
    print("\nüìà Creating weekly aggregates...")
    weekly_agg = full_df.groupby('week_date').agg({
        'weekly_sales': 'sum',
        'isholiday': 'sum'  # Count of holiday weeks
    }).reset_index()
    weekly_agg.columns = ['week_date', 'total_sales', 'n_holiday_stores']
    weekly_agg.to_csv(TABLES_DIR / "dashboard_weekly_totals.csv", index=False)
    print(f"  ‚úÖ Saved: {TABLES_DIR / 'dashboard_weekly_totals.csv'}")
    
    # 2. Store-level weekly aggregates
    store_weekly = full_df.groupby(['week_date', 'store_id']).agg({
        'weekly_sales': 'sum',
        'store_type': 'first',
        'store_size': 'first'
    }).reset_index()
    store_weekly.to_csv(TABLES_DIR / "dashboard_store_weekly.csv", index=False)
    print(f"  ‚úÖ Saved: {TABLES_DIR / 'dashboard_store_weekly.csv'}")
    
    # 3. Dept-level weekly aggregates
    dept_weekly = full_df.groupby(['week_date', 'dept_id']).agg({
        'weekly_sales': 'sum'
    }).reset_index()
    dept_weekly.to_csv(TABLES_DIR / "dashboard_dept_weekly.csv", index=False)
    print(f"  ‚úÖ Saved: {TABLES_DIR / 'dashboard_dept_weekly.csv'}")
    
    # 4. Top-N departments by volume
    dept_volume = full_df.groupby('dept_id')['weekly_sales'].sum().sort_values(ascending=False).reset_index()
    dept_volume.columns = ['dept_id', 'total_sales']
    dept_volume['rank'] = range(1, len(dept_volume) + 1)
    dept_volume.to_csv(TABLES_DIR / "dashboard_top_depts_by_volume.csv", index=False)
    print(f"  ‚úÖ Saved: {TABLES_DIR / 'dashboard_top_depts_by_volume.csv'}")
    
    # 5. Top-N departments by volatility
    series_stats = pd.read_csv(TABLES_DIR / "series_statistics.csv")
    dept_volatility = series_stats.groupby('dept_id')['cv'].mean().sort_values(ascending=False).reset_index()
    dept_volatility.columns = ['dept_id', 'avg_cv']
    dept_volatility['rank'] = range(1, len(dept_volatility) + 1)
    dept_volatility.to_csv(TABLES_DIR / "dashboard_top_depts_by_volatility.csv", index=False)
    print(f"  ‚úÖ Saved: {TABLES_DIR / 'dashboard_top_depts_by_volatility.csv'}")


def main():
    """Main EDA function."""
    print("=" * 60)
    print("üîç Exploratory Data Analysis (EDA)")
    print("=" * 60)
    
    # Load data
    train_df, val_df, test_df, full_df = load_data()
    
    # Run all analyses
    schema_sanity_checks(full_df)
    time_series_shape_analysis(full_df)
    series_level_diagnostics(full_df)
    dashboard_ready_aggregates(full_df)
    
    print("\n" + "=" * 60)
    print("‚úÖ EDA Complete!")
    print("=" * 60)
    print(f"\nüìÅ Outputs saved to:")
    print(f"  - Figures: {FIGURES_DIR}")
    print(f"  - Tables: {TABLES_DIR}")
    print("\nüí° Next steps:")
    print("  1. Review plots and tables to understand data patterns")
    print("  2. Use insights to refine feature engineering")
    print("  3. Use insights to strengthen baselines")


if __name__ == "__main__":
    main()
